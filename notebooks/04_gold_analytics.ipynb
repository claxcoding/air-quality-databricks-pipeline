{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "finishTime": 1769777803536,
     "inputWidgets": {},
     "nuid": "fb24818f-8a9f-4fe3-a88f-a057a035e38d",
     "showTitle": true,
     "startTime": 1769777790552,
     "submitTime": 1769777790494,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "04_gold_analytics.py\n",
    "\n",
    "Purpose:\n",
    "- Aggregate Silver sensor measurements\n",
    "- Produce daily, location-level and latest sensor snapshot air quality metrics\n",
    "- Create analytics-ready Gold Delta table for python engineer analytics\n",
    "- Create analytics-ready Gold Delta table for dashboard analytics\n",
    "\n",
    "Input:\n",
    "- air_quality_silver.sensor_measurements\n",
    "\n",
    "Output:\n",
    "- air_quality_gold.daily_air_quality\n",
    "- air_quality_gold.latest_sensor_snapshot\n",
    "\"\"\"\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    to_date,\n",
    "    avg,\n",
    "    count,\n",
    "    countDistinct,\n",
    "    when,\n",
    "    max as spark_max\n",
    ")\n",
    "\n",
    "# Ensure Gold database exists\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS air_quality_gold\")\n",
    "\n",
    "# Read Silver table\n",
    "silver_df = spark.read.table(\"air_quality_silver.sensor_measurements\")\n",
    "\n",
    "# Filter PM measurements only\n",
    "pm_df = silver_df.filter(\n",
    "    col(\"measurement_type\").isin(\"P1\", \"P2\")\n",
    ")\n",
    "\n",
    "# Add date column\n",
    "pm_df = pm_df.withColumn(\n",
    "    \"date\",\n",
    "    to_date(col(\"measurement_ts\"))\n",
    ")\n",
    "\n",
    "# Aggregate to location level\n",
    "gold_df = (\n",
    "    pm_df.groupBy(\n",
    "        \"date\",\n",
    "        \"location_id\",\n",
    "        \"country\",\n",
    "        \"latitude\",\n",
    "        \"longitude\"\n",
    "    )\n",
    "    .agg(\n",
    "        avg(\n",
    "            when(col(\"measurement_type\") == \"P1\", col(\"measurement_value\"))\n",
    "        ).alias(\"pm10_avg\"),\n",
    "\n",
    "        avg(\n",
    "            when(col(\"measurement_type\") == \"P2\", col(\"measurement_value\"))\n",
    "        ).alias(\"pm25_avg\"),\n",
    "\n",
    "        count(\n",
    "            when(col(\"measurement_type\") == \"P1\", col(\"measurement_value\"))\n",
    "        ).alias(\"pm10_count\"),\n",
    "\n",
    "        count(\n",
    "            when(col(\"measurement_type\") == \"P2\", col(\"measurement_value\"))\n",
    "        ).alias(\"pm25_count\"),\n",
    "\n",
    "        countDistinct(\"sensor_id\").alias(\"sensors\"),\n",
    "        count(\"*\").alias(\"measurements\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Pivot PM values into columns for dashboard analytics\n",
    "pm_pivot_df = (\n",
    "    pm_df\n",
    "    .groupBy(\n",
    "        \"sensor_id\",\n",
    "        \"sensor_type\",\n",
    "        \"location_id\",\n",
    "        \"country\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"measurement_ts\",\n",
    "        \"ingested_at\"\n",
    "    )\n",
    "    .pivot(\"measurement_type\", [\"P1\", \"P2\"])\n",
    "    .agg(spark_max(\"measurement_value\"))\n",
    "    .withColumnRenamed(\"P1\", \"pm10\")\n",
    "    .withColumnRenamed(\"P2\", \"pm25\")\n",
    "    .filter(col(\"pm10\").isNotNull() | col(\"pm25\").isNotNull())\n",
    ")\n",
    "\n",
    "# Pick the latest row per sensor (snapshot step)\n",
    "window_spec = Window.partitionBy(\"sensor_id\").orderBy(\n",
    "    col(\"measurement_ts\").desc()\n",
    ")\n",
    "\n",
    "latest_df = (\n",
    "    pm_pivot_df\n",
    "    .withColumn(\"rn\", row_number().over(window_spec))\n",
    "    .filter(col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "# Write Gold table for python sanity checks\n",
    "GOLD_TABLE_DAILY = \"air_quality_gold.daily_air_quality\"\n",
    "\n",
    "gold_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(GOLD_TABLE_DAILY)\n",
    "\n",
    "gold_rows_python = spark.read.table(\n",
    "    \"air_quality_gold.daily_air_quality\"\n",
    ").count()\n",
    "\n",
    "# Sanity check python\n",
    "display(\n",
    "    spark.sql(\n",
    "        f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {GOLD_TABLE_DAILY}\n",
    "        ORDER BY date DESC\n",
    "        LIMIT 100\n",
    "        \"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Write Gold latest snaphot table for dashboard analytics\n",
    "GOLD_TABLE_SNAPSHOT = \"air_quality_gold.latest_sensor_snapshot\"\n",
    "\n",
    "latest_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(GOLD_TABLE_SNAPSHOT)\n",
    "\n",
    "gold_rows_dashboard = spark.read.table(\n",
    "    \"air_quality_gold.latest_sensor_snapshot\"\n",
    ").count()\n",
    "\n",
    "# Sanity check dashboard\n",
    "display(\n",
    "    spark.sql(\n",
    "        f\"\"\"\n",
    "        SELECT\n",
    "            COUNT(*) AS sensors,\n",
    "            AVG(pm25) AS avg_pm25,\n",
    "            MAX(ingested_at) AS last_ingest\n",
    "        FROM {GOLD_TABLE_SNAPSHOT}\n",
    "        \"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display gold rows for python and dashboard from completed steps in the orchestration\n",
    "dbutils.notebook.exit(\n",
    "    f\"Gold aggregation python sanity check completed: {gold_rows_python} records for python produced \\nGold aggregation for the dashboard completed: {gold_rows_dashboard} records for the dashboard produced\"\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_gold_analytics",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}