{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "finishTime": 1770212365359,
     "inputWidgets": {},
     "nuid": "56afa9b3-4f9f-43df-a1e8-017e4f3e92b8",
     "showTitle": true,
     "startTime": 1770212356827,
     "submitTime": 1770212356794,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "02_bronze_ingest_live.py\n",
    "\n",
    "Purpose:\n",
    "- Ingest raw live sensor data from sensor.community\n",
    "- Store unmodified JSON payloads in Bronze Delta table\n",
    "- Append-only ELT ingestion\n",
    "\"\"\"\n",
    "\n",
    "# Databricks notebooks do not automatically include the project root in PYTHONPATH.\n",
    "# To enable imports from src/, each notebook adds the project root to sys.path at runtime\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import json\n",
    "from src.utils.http import fetch_json\n",
    "from src.utils.bronze import prepare_bronze_rows\n",
    "\n",
    "SENSOR_URL: str = \"https://data.sensor.community/static/v2/data.json\"\n",
    "SOURCE_NAME: str = \"sensor.community\"\n",
    "\n",
    "# Ensure Bronze schema exists (idempotent)\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS air_quality_bronze\")\n",
    "\n",
    "# Fetch live sensor data\n",
    "records = fetch_json(SENSOR_URL)\n",
    "\n",
    "# Prepare rows for Bronze table\n",
    "bronze_rows = prepare_bronze_rows(records, SOURCE_NAME)\n",
    "\n",
    "# Create Spark DataFrame with explicit schema\n",
    "bronze_df = spark.createDataFrame(\n",
    "    bronze_rows,\n",
    "    schema = [\"raw_json\", \"ingested_at\", \"source\", \"batch_id\"]\n",
    ")\n",
    "\n",
    "BRONZE_TABLE = \"air_quality_bronze.live_sensor_raw\"\n",
    "\n",
    "# Append new data to Bronze Delta table\n",
    "bronze_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(BRONZE_TABLE)\n",
    "\n",
    "# Bronze layer sanity check: verify batch-level ingestion and append behavior\n",
    "display(\n",
    "    spark.sql(\n",
    "        f\"\"\"\n",
    "        SELECT\n",
    "            source,\n",
    "            batch_id,\n",
    "            COUNT(*) AS records,\n",
    "            MAX(ingested_at) AS latest_ingested_at\n",
    "        FROM {BRONZE_TABLE}\n",
    "        GROUP BY source, batch_id\n",
    "        ORDER BY latest_ingested_at DESC \n",
    "        \"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Print the first row of the DataFrame as JSON\n",
    "first_row = bronze_df.collect()[0].asDict()\n",
    "first_row[\"ingested_at\"] = first_row[\"ingested_at\"].isoformat()\n",
    "print(json.dumps(first_row, indent=2))\n",
    "\n",
    "rows_ingested = bronze_df.count()\n",
    "\n",
    "dbutils.notebook.exit(\n",
    "    f\"Bronze ingestion completed: {rows_ingested} records ingested\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_bronze_ingest_live",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}