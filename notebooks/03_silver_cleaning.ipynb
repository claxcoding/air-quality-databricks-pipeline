{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "finishTime": 1769777557219,
     "inputWidgets": {},
     "nuid": "41f2c144-c471-49b9-a3f3-c0aa75c9b57a",
     "showTitle": true,
     "startTime": 1769777534629,
     "submitTime": 1769777534595,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "03_silver_cleaning.py\n",
    "\n",
    "Purpose:\n",
    "- Parse raw JSON from Bronze layer\n",
    "- Enforce schema and data types\n",
    "- Clean and deduplicate sensor measurements\n",
    "- Write structured Silver Delta table\n",
    "\n",
    "Input:\n",
    "- air_quality_bronze.live_sensor_raw\n",
    "\n",
    "Output:\n",
    "- air_quality_silver.sensor_measurements\n",
    "\n",
    "Visual tree view of the sensor_schema:\n",
    "sensor_schema\n",
    "├─ id : Long\n",
    "├─ timestamp : String\n",
    "├─ location : Struct\n",
    "│    ├─ id : Long\n",
    "│    ├─ latitude : String\n",
    "│    ├─ longitude : String\n",
    "│    └─ country : String\n",
    "├─ sensor : Struct\n",
    "│    ├─ id : Long\n",
    "│    └─ sensor_type : Struct\n",
    "│          └─ name : String\n",
    "└─ sensordatavalues : Array of Struct\n",
    "     ├─ value_type : String\n",
    "     └─ value : String\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Ensure silver database exists\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS air_quality_silver\")\n",
    "\n",
    "# Read bronze table\n",
    "from pyspark.sql.functions import (\n",
    "    col, \n",
    "    from_json, \n",
    "    to_timestamp, \n",
    "    explode, \n",
    "    to_timestamp, \n",
    "    first, \n",
    "    when)\n",
    "from pyspark.sql.types import (\n",
    "    StructType, \n",
    "    StructField, \n",
    "    StringType, \n",
    "    DoubleType, \n",
    "    LongType, \n",
    "    ArrayType)\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "bronze_df = spark.read.table(\"air_quality_bronze.live_sensor_raw\")\n",
    "\n",
    "# Define JSON schema\n",
    "sensor_schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),\n",
    "\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    \n",
    "    StructField(\"location\", StructType([\n",
    "        StructField(\"id\", LongType(), True),\n",
    "        StructField(\"latitude\", StringType(), True),\n",
    "        StructField(\"longitude\", StringType(), True),\n",
    "        StructField(\"country\", StringType(), True)\n",
    "    ]), True),\n",
    "\n",
    "    StructField(\"sensor\", StructType([\n",
    "        StructField(\"id\", LongType(), True),\n",
    "        StructField(\"sensor_type\", StructType([\n",
    "            StructField(\"name\", StringType(), True)\n",
    "        ]), True)\n",
    "    ]), True),\n",
    "\n",
    "    StructField(\n",
    "        \"sensordatavalues\",\n",
    "        ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"value_type\", StringType(), True),\n",
    "                StructField(\"value\", StringType(), True)\n",
    "            ])\n",
    "        ),\n",
    "        True\n",
    "    )\n",
    "])\n",
    "\n",
    "# Parse JSON & explode onto structured columns\n",
    "parsed_df = bronze_df.withColumn(\n",
    "    \"parsed_json\",\n",
    "    from_json(col(\"raw_json\"), sensor_schema)\n",
    ")\n",
    "\n",
    "# Select & clean silver columns\n",
    "exploded_df = parsed_df.select(\n",
    "    col(\"parsed_json.sensor.id\").alias(\"sensor_id\"),\n",
    "    col(\"parsed_json.sensor.sensor_type.name\").alias(\"sensor_type\"),\n",
    "    to_timestamp(col(\"parsed_json.timestamp\")).alias(\"measurement_ts\"),\n",
    "    col(\"parsed_json.location.id\").alias(\"location_id\"),\n",
    "    col(\"parsed_json.location.latitude\").cast(\"double\").alias(\"latitude\"),\n",
    "    col(\"parsed_json.location.longitude\").cast(\"double\").alias(\"longitude\"),\n",
    "    col(\"parsed_json.location.country\").alias(\"country\"),\n",
    "    explode(col(\"parsed_json.sensordatavalues\")).alias(\"measurement\"),\n",
    "    col(\"ingested_at\"),\n",
    "    col(\"batch_id\")\n",
    ")\n",
    "\n",
    "# Silver projection\n",
    "silver_df = exploded_df.select(\n",
    "    \"sensor_id\",\n",
    "    \"sensor_type\",\n",
    "    \"measurement_ts\",\n",
    "    \"location_id\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"country\",\n",
    "    col(\"measurement.value_type\").alias(\"measurement_type\"),\n",
    "    col(\"measurement.value\").cast(\"double\").alias(\"measurement_value\"),\n",
    "    \"ingested_at\",\n",
    "    \"batch_id\"\n",
    ").filter(\n",
    "    col(\"measurement_ts\").isNotNull() &\n",
    "    col(\"measurement_value\").isNotNull()\n",
    ")\n",
    "\n",
    "# Filter invalid records (light cleaning)\n",
    "silver_df_clean = silver_df.filter(\n",
    "    col(\"sensor_id\").isNotNull() &\n",
    "    col(\"measurement_ts\").isNotNull()\n",
    ")\n",
    "\n",
    "# Deduplicate\n",
    "silver_df_dedup = silver_df_clean.dropDuplicates(\n",
    "    [\"sensor_id\", \"measurement_ts\", \"measurement_type\"]\n",
    ")\n",
    "\n",
    "# Add quality checks / plausibility filtering\n",
    "# Define physical plausibility\n",
    "silver_df_filtered = silver_df_dedup.withColumn(\n",
    "    \"is_plausible\",\n",
    "    (col(\"measurement_value\") >= 0)\n",
    ")\n",
    "\n",
    "# Create separate PM10 / PM2.5 columns to check ratio\n",
    "pm_df = silver_df_filtered.filter(col(\"measurement_type\").isin([\"P1\", \"P2\"]))\n",
    "\n",
    "pm_pivot_df = pm_df.groupBy(\n",
    "    \"location_id\",\n",
    "    \"measurement_ts\",\n",
    ").pivot(\"measurement_type\", [\"P1\", \"P2\"]).agg(first(\"measurement_value\"))\n",
    "\n",
    "# Add plausibility flag for ratio: PM2.5 <= 1.2 * PM10\n",
    "pm_pivot_df = pm_pivot_df.withColumn(\n",
    "    \"ratio_plausible\",\n",
    "    (col(\"P2\") <= col(\"P1\")*1.2)\n",
    ")\n",
    "\n",
    "# Merge back ratio flag with main silver_df\n",
    "silver_df_final = silver_df_filtered.join(\n",
    "    pm_pivot_df.select(\"location_id\", \"measurement_ts\", \"ratio_plausible\"),\n",
    "    on=[\"location_id\", \"measurement_ts\"],\n",
    "    how=\"left\"\n",
    ").withColumn(\n",
    "    \"quality_flag\",\n",
    "    when(col(\"is_plausible\") & col(\"ratio_plausible\"), \"OK\").otherwise(\"BAD\")         \n",
    ")\n",
    "\n",
    "# Write Silver Delta table\n",
    "SILVER_TABLE = \"air_quality_silver.sensor_measurements\"\n",
    "\n",
    "silver_df_final.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(SILVER_TABLE)\n",
    "\n",
    "# Silver layer schema check\n",
    "display(\n",
    "    spark.sql(\n",
    "        f\"\"\"\n",
    "        DESCRIBE {SILVER_TABLE}\"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Silver layer cardinality check\n",
    "display(\n",
    "    spark.sql(\n",
    "        f\"\"\"\n",
    "        SELECT\n",
    "            COUNT(*) AS rows,\n",
    "            COUNT(DISTINCT sensor_id) AS sensors,\n",
    "            COUNT(DISTINCT location_id) AS locations\n",
    "        FROM {SILVER_TABLE}\"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Silver layer sanity check 1: Verify which sensors exist & whether PM-data exists\n",
    "display(\n",
    "    spark.sql(\n",
    "        f\"\"\"\n",
    "        SELECT\n",
    "            measurement_type,\n",
    "            COUNT(*) AS records\n",
    "        FROM {SILVER_TABLE}\n",
    "        GROUP BY measurement_type\n",
    "        ORDER BY records DESC\n",
    "        \"\"\"            \n",
    "    )\n",
    ")\n",
    "\n",
    "# Silver layer sanity check 2: Check PM-only aggregation\n",
    "display(\n",
    "    spark.sql(\n",
    "        f\"\"\"\n",
    "        SELECT\n",
    "            sensor_type,\n",
    "            measurement_type,\n",
    "            COUNT(*) AS records,\n",
    "            AVG(measurement_value) AS avg_value\n",
    "        FROM {SILVER_TABLE}\n",
    "        WHERE measurement_type IN ('P1', 'P2')\n",
    "        GROUP BY sensor_type, measurement_type\n",
    "        ORDER BY records DESC\n",
    "        \"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "silver_rows = spark.read.table(\n",
    "    \"air_quality_silver.sensor_measurements\"\n",
    ").count()\n",
    "\n",
    "dbutils.notebook.exit(\n",
    "    f\"Silver cleaning completed: {silver_rows} records in silver table\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_silver_cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}